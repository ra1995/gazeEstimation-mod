{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from tqdm.notebook import tqdm\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, imutils\n",
    "from PIL import Image\n",
    "import json, yaml, collections\n",
    "import pickle as pkl\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import functional as F\n",
    "from torchvision import transforms as T\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from typing import List\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptgaze.common import Camera, Face, FacePartsName, Visualizer\n",
    "from ptgaze.utils import get_3d_face_model\n",
    "from ptgaze.head_pose_estimation.head_pose_normalizer import HeadPoseNormalizer\n",
    "from ptgaze.models import create_model\n",
    "from ptgaze.transforms import create_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeEstimator:\n",
    "    EYE_KEYS = [FacePartsName.REYE, FacePartsName.LEYE]\n",
    "\n",
    "    def __init__(self, config: DictConfig):\n",
    "        self._config = config\n",
    "        self._face_model3d = get_3d_face_model(config)\n",
    "        self.camera = Camera(config.gaze_estimator.camera_params)\n",
    "        self._normalized_camera = Camera(\n",
    "            config.gaze_estimator.normalized_camera_params)\n",
    "\n",
    "        # self._landmark_estimator = LandmarkEstimator(config)\n",
    "        self._head_pose_normalizer = HeadPoseNormalizer(\n",
    "            self.camera, self._normalized_camera,\n",
    "            self._config.gaze_estimator.normalized_camera_distance)\n",
    "\n",
    "        self.device = torch.device(self._config.device)\n",
    "        self._gaze_estimation_model = self._load_model()\n",
    "        self._transform = create_transform(config)\n",
    "\n",
    "    def _load_model(self) -> torch.nn.Module:\n",
    "        model = create_model(self._config)\n",
    "        checkpoint = torch.load(self._config.gaze_estimator.checkpoint,\n",
    "                                map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def detect_faces(self, image: np.ndarray, landmarks_arr:list) -> List[Face]:\n",
    "        # return self._landmark_estimator.detect_faces(image)\n",
    "        detected = []\n",
    "        for l_idx in range(len(landmarks_arr)):\n",
    "            pts = landmarks_arr[l_idx][:, :2]*np.array([[image.shape[1], image.shape[0]]])\n",
    "            bbox = np.vstack([pts.min(axis=0), pts.max(axis=0)])\n",
    "            bbox = np.round(bbox).astype(np.int32)\n",
    "            detected.append(Face(bbox, pts))\n",
    "        return detected\n",
    "\n",
    "    def estimate_gaze(self, image: np.ndarray, face: Face) -> None:\n",
    "        self._face_model3d.estimate_head_pose(face, self.camera)\n",
    "        self._face_model3d.compute_3d_pose(face)\n",
    "        self._face_model3d.compute_face_eye_centers(face, self._config.mode)\n",
    "\n",
    "        if self._config.mode == 'MPIIGaze':\n",
    "            for key in self.EYE_KEYS:\n",
    "                eye = getattr(face, key.name.lower())\n",
    "                self._head_pose_normalizer.normalize(image, eye)\n",
    "            self._run_mpiigaze_model(face)\n",
    "        elif self._config.mode == 'MPIIFaceGaze':\n",
    "            self._head_pose_normalizer.normalize(image, face)\n",
    "            self._run_mpiifacegaze_model(face)\n",
    "        elif self._config.mode == 'ETH-XGaze':\n",
    "            self._head_pose_normalizer.normalize(image, face)\n",
    "            self._run_ethxgaze_model(face)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _run_mpiigaze_model(self, face: Face) -> None:\n",
    "        images = []\n",
    "        head_poses = []\n",
    "        for key in self.EYE_KEYS:\n",
    "            eye = getattr(face, key.name.lower())\n",
    "            image = eye.normalized_image\n",
    "            normalized_head_pose = eye.normalized_head_rot2d\n",
    "            if key == FacePartsName.REYE:\n",
    "                image = image[:, ::-1].copy()\n",
    "                normalized_head_pose *= np.array([1, -1])\n",
    "            image = self._transform(image)\n",
    "            images.append(image)\n",
    "            head_poses.append(normalized_head_pose)\n",
    "        images = torch.stack(images)\n",
    "        head_poses = np.array(head_poses).astype(np.float32)\n",
    "        head_poses = torch.from_numpy(head_poses)\n",
    "\n",
    "        images = images.to(self.device)\n",
    "        head_poses = head_poses.to(self.device)\n",
    "        predictions = self._gaze_estimation_model(images, head_poses)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "\n",
    "        for i, key in enumerate(self.EYE_KEYS):\n",
    "            eye = getattr(face, key.name.lower())\n",
    "            eye.normalized_gaze_angles = predictions[i]\n",
    "            if key == FacePartsName.REYE:\n",
    "                eye.normalized_gaze_angles *= np.array([1, -1])\n",
    "            # eye.angle_to_vector()\n",
    "            # eye.denormalize_gaze_vector()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _run_mpiifacegaze_model(self, face: Face) -> None:\n",
    "        image = self._transform(face.normalized_image).unsqueeze(0)\n",
    "\n",
    "        image = image.to(self.device)\n",
    "        prediction = self._gaze_estimation_model(image)\n",
    "        prediction = prediction.cpu().numpy()\n",
    "\n",
    "        face.normalized_gaze_angles = prediction[0]\n",
    "        # face.angle_to_vector()\n",
    "        # face.denormalize_gaze_vector()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _run_ethxgaze_model(self, face: Face) -> None:\n",
    "        image = self._transform(face.normalized_image).unsqueeze(0)\n",
    "\n",
    "        image = image.to(self.device)\n",
    "        prediction = self._gaze_estimation_model(image)\n",
    "        prediction = prediction.cpu().numpy()\n",
    "\n",
    "        face.normalized_gaze_angles = prediction[0]\n",
    "        # face.angle_to_vector()\n",
    "        # face.denormalize_gaze_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_camera_params(config: DictConfig, cap=None, downscale=1) -> None:\n",
    "    if cap is None:\n",
    "        w = int(1280/downscale)\n",
    "        h = int(720/downscale)\n",
    "    elif type(cap)==cv2.VideoCapture:\n",
    "        w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)/downscale), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)/downscale)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    out_file = config.gaze_estimator.camera_params\n",
    "    c_dict = {\n",
    "        'image_width': w,\n",
    "        'image_height': h,\n",
    "        'camera_matrix': {\n",
    "            'rows': 3,\n",
    "            'cols': 3,\n",
    "            'data': [w, 0., w / 2, 0., w, h / 2, 0., 0., 1.]\n",
    "        },\n",
    "        'distortion_coefficients': {\n",
    "            'rows': 1,\n",
    "            'cols': 5,\n",
    "            'data': [0., 0., 0., 0., 0.]\n",
    "        }\n",
    "    }\n",
    "    with open(out_file, 'w') as fp:\n",
    "        yaml.safe_dump(c_dict, fp)\n",
    "        # yaml.dump(c_dict, fp, default_flow_style = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_gaze_perpective(frame, x1, y1, rot_mat, dist=100., color=(0,0,255), thickness=2):\n",
    "    focal_length = frame.shape[1]/2\n",
    "    camera_origin = np.array([0., 0., focal_length], dtype=float)\n",
    "    camera_center = (frame.shape[1] / 2, frame.shape[0] / 2)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0., camera_center[0]],\n",
    "        [0., focal_length, camera_center[1]],\n",
    "        [0., 0., 1.]], dtype=float)\n",
    "    # print(camera_matrix)\n",
    "    # print(x1, y1)\n",
    "    \n",
    "    src_pts = np.array([[0., 0., 1e-5, 1.], [0., 0., dist, 1.]], dtype=float)\n",
    "    Tmat = np.identity(4, dtype=float)\n",
    "    Tmat[:3, :3] = rot_mat\n",
    "    Tmat[:3, 3] = np.array([x1, y1, 0])\n",
    "    dst_pts = (Tmat @ src_pts.T).T\n",
    "    dst_pts[:, :3] -= camera_origin\n",
    "    dst_pts[:, 2] *= -1\n",
    "    dst_pts /= dst_pts[:, [2]]\n",
    "    img_pts = (camera_matrix @ dst_pts[:, :3].T).T\n",
    "    # img_pts /= img_pts[:, [2]]\n",
    "    \n",
    "    img = cv2.arrowedLine(frame, tuple(np.round(img_pts[0, :2]).astype(np.int32)),\n",
    "                   tuple(np.round(img_pts[1, :2]).astype(int)), color,\n",
    "                   thickness, cv2.LINE_AA, tipLength=0.18)\n",
    "    return img\n",
    "\n",
    "def disp_gaze_ROIperpective(frame, x1, y1, cx, cy, width, height, rot_mat, dist=100., color=(0,0,255), thickness=2):\n",
    "    focal_length = frame.shape[1]/2\n",
    "    camera_origin = np.array([0, 0, focal_length], dtype=float)\n",
    "    camera_center = (width / 2, height / 2)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0., camera_center[0]],\n",
    "        [0., focal_length, camera_center[1]],\n",
    "        [0., 0., 1.]], dtype=float)\n",
    "    # print(camera_matrix)\n",
    "    # print(x1, y1)\n",
    "    \n",
    "    src_pts = np.array([[0., 0., 1e-5, 1.], [0., 0., dist, 1.]], dtype=float)\n",
    "    Tmat = np.identity(4, dtype=float)\n",
    "    Tmat[:3, :3] = rot_mat\n",
    "    Tmat[:3, 3] = np.array([x1-width/2, y1-height/2, 0])\n",
    "    dst_pts = (Tmat @ src_pts.T).T\n",
    "    dst_pts[:, :3] -= camera_origin\n",
    "    dst_pts[:, 2] *= -1\n",
    "    dst_pts /= dst_pts[:, [2]]\n",
    "    img_pts = (camera_matrix @ dst_pts[:, :3].T).T\n",
    "    img_pts -= np.array([[-cx, -cy, 0]])\n",
    "    \n",
    "    img = cv2.arrowedLine(frame, tuple(np.round(img_pts[0, :2]).astype(np.int32)),\n",
    "                   tuple(np.round(img_pts[1, :2]).astype(int)), color,\n",
    "                   thickness, cv2.LINE_AA, tipLength=0.18)\n",
    "    return img\n",
    "\n",
    "def disp_gaze_orthographic(frame, x1, y1, rot_mat, dist=100., color=(0,0,255), thickness=2):\n",
    "    focal_length = frame.shape[1]/2\n",
    "    camera_origin = np.array([0., 0., focal_length], dtype=float)\n",
    "    camera_center = (frame.shape[1] / 2, frame.shape[0] / 2)\n",
    "    \n",
    "    src_pts = np.array([[0., 0., 1e-5, 1.], [0., 0., dist, 1.]], dtype=float)\n",
    "    Tmat = np.identity(4, dtype=float)\n",
    "    Tmat[:3, :3] = rot_mat\n",
    "    Tmat[:3, 3] = np.array([x1, y1, 0])\n",
    "    dst_pts = (Tmat @ src_pts.T).T\n",
    "    dst_pts[:, :3] -= camera_origin\n",
    "    dst_pts[:, 2] *= -1\n",
    "\n",
    "    img = cv2.arrowedLine(frame, tuple(np.round(dst_pts[0, :2]).astype(np.int32)),\n",
    "                   tuple(np.round(dst_pts[1, :2]).astype(int)), color,\n",
    "                   thickness, cv2.LINE_AA, tipLength=0.18)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_eye = [33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7]\n",
    "right_eye = [362, 398, 384, 385, 386, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381, 382]\n",
    "nose_tip = [1, 4]\n",
    "left_lipend = [61, 76]\n",
    "right_lipend = [306, 291]\n",
    "landmark_keys = [left_eye, right_eye, nose_tip, left_lipend, right_lipend]\n",
    "landmark_color = [(255, 0, 64), (255, 64, 0), (0, 255, 0), (64, 0, 255), (0, 64, 255)]\n",
    "\n",
    "def get_five_landmarks(frame, landmarks):\n",
    "    pts = []\n",
    "    for idx in range(len(landmark_keys)):\n",
    "        vertices = landmarks[landmark_keys[idx], :]\n",
    "        pt = np.mean(vertices[:, :2], axis=0)*np.array([frame.shape[1], frame.shape[0]])\n",
    "        pts.append(pt)\n",
    "    pts = np.stack(pts)\n",
    "    return pts\n",
    "\n",
    "def draw_landmarks(frame, landmarks):\n",
    "    for idx in range(landmarks.shape[0]):\n",
    "        cv2.circle(frame, (*landmarks[idx].astype(int),), 3, landmark_color[idx], -1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['code.py', '-use_dummy_camera_params']\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('-mode', metavar='mode', default='eth-xgaze', type=str, help='{mpiigaze,mpiifacegaze,eth-xgaze}')\n",
    "parser.add_argument('-model', metavar='model', default='./ptgaze/models/eth-xgaze_resnet18.pth', type=str)\n",
    "parser.add_argument('-config', metavar='config', default='./ptgaze/data/configs/eth-xgaze.yaml', type=str)\n",
    "parser.add_argument('-device', metavar='device', default='cuda:0', type=str)\n",
    "parser.add_argument('-use_dummy_camera_params', action='store_true')\n",
    "args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PACKAGE_ROOT'] = 'C:\\\\Users\\\\Rishabh\\\\Documents\\\\Repos\\\\face\\\\Gaze\\\\gazeEstimation\\\\ptgaze'\n",
    "config = OmegaConf.load(args.config)\n",
    "config['PACKAGE_ROOT'] = './ptgaze'\n",
    "config['gaze_estimator'].camera_params = config['gaze_estimator'].camera_params.format(config['PACKAGE_ROOT'])\n",
    "config['gaze_estimator'].normalized_camera_params = config['gaze_estimator'].normalized_camera_params.format(config['PACKAGE_ROOT'])\n",
    "config['gaze_estimator'].checkpoint = args.model\n",
    "config.device = args.device\n",
    "\n",
    "config.gaze_estimator.use_dummy_camera_params = args.use_dummy_camera_params\n",
    "if config.gaze_estimator.use_dummy_camera_params:\n",
    "    generate_dummy_camera_params(config)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'D:/Datasets/JALI/'\n",
    "video_dir = join(root, 'reencoded')\n",
    "video_names = [f for f in listdir(video_dir) if isfile(join(video_dir, f))]\n",
    "video_names.sort()\n",
    "print('\\n'.join(video_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_flag = False\n",
    "pose_dir = join(root, 'pose')\n",
    "save_dir = join(root, 'ETHGaze')\n",
    "if not isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "fixed_height = 480\n",
    "downsample = 1\n",
    "\n",
    "for vid_num in tqdm(range(len(video_names))):\n",
    "    vid_prefix = video_names[vid_num].split('.mp4')[0]\n",
    "    vid_path = join(video_dir, video_names[vid_num])\n",
    "    pose_path = join(pose_dir, vid_prefix+'.pkl')\n",
    "    \n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps, frame_count = cap.get(cv2.CAP_PROP_FPS), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(vid_prefix)\n",
    "    print('FPS:{},RES:{}x{}|COUNT:{}'.format(fps, width, height, frame_count))\n",
    "    \n",
    "    pose_writer = None\n",
    "    save_path = join(save_dir, vid_prefix+'.mp4')\n",
    "    downscale = height/fixed_height\n",
    "    generate_dummy_camera_params(config, cap, downscale)\n",
    "\n",
    "    gaze_estimator = GazeEstimator(config)\n",
    "    gaze_estimator.camera.camera_matrix = gaze_estimator.camera.camera_matrix.astype(np.float32)\n",
    "    gaze_estimator.camera.dist_coefficients = gaze_estimator.camera.dist_coefficients.reshape(5).astype(np.float32)\n",
    "    print(gaze_estimator.camera.camera_matrix, gaze_estimator.camera.dist_coefficients)\n",
    "    \n",
    "    prev_rotation=None\n",
    "    deltatime = 0\n",
    "    kf_obj = cv2.KalmanFilter(4, 2, 0)\n",
    "    kf_obj.transitionMatrix = np.array([[1,0,1,0], [0,1,0,1], [0,0,1,0], [0,0,0,1]], dtype=np.float32)\n",
    "    kf_obj.measurementMatrix = np.array([[1,0,0,0], [0,1,0,0]], dtype=np.float32)\n",
    "\n",
    "    pose_dict = {}\n",
    "    with open(pose_path, 'rb') as fp:\n",
    "        pose_dict = pkl.load(fp)\n",
    "    landmarks_arr = pose_dict['LANDMARKS']\n",
    "    faces_arr = pose_dict['BBOX']\n",
    "    \n",
    "    nface_dict = {}\n",
    "    raw_faces = []\n",
    "    raw_landmarks = []\n",
    "    raw_rot_eulers = []\n",
    "    raw_rot_matrices = []\n",
    "    smooth_rot_eulers = []\n",
    "    smooth_rot_matrices = []\n",
    "\n",
    "    for frame_num in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_resized = imutils.resize(frame, height=fixed_height)\n",
    "        undistorted_frame = cv2.undistort(\n",
    "                                frame_resized, gaze_estimator.camera.camera_matrix,\n",
    "                                gaze_estimator.camera.dist_coefficients)\n",
    "        frame_copy = frame_resized.copy()\n",
    "        \n",
    "        # nface_dict[str(frame_num)] = []\n",
    "        raw_face = faces_arr[frame_num]#*np.array([width, height, width, height])\n",
    "        raw_bbox = (raw_face/downscale).astype(int)\n",
    "        raw_bbox = (raw_bbox[0], raw_bbox[1], raw_bbox[2]-raw_bbox[0], raw_bbox[3]-raw_bbox[1])\n",
    "        face_ar = raw_bbox[2]*raw_bbox[3]\n",
    "        start_time = time.time()\n",
    "\n",
    "        if face_ar>0:\n",
    "            landmark_pts = landmarks_arr[frame_num].astype(np.float32)\n",
    "            five_landmarks = get_five_landmarks(frame, landmark_pts)/downscale\n",
    "            # nface_dict[str(frame_num)] = [raw_face, five_landmarks.astype(float)]\n",
    "            raw_faces.append(raw_face.astype(np.float32))\n",
    "            raw_landmarks.append(landmark_pts[:468, :2]*np.array([[frame.shape[1], frame.shape[0]]]))\n",
    "\n",
    "            faces_list = gaze_estimator.detect_faces(undistorted_frame, [landmark_pts[:468]])\n",
    "            cv2.rectangle(frame_copy, raw_bbox, (0,0,255), 2)\n",
    "            cv2.circle(frame_copy, (int(five_landmarks[2, 0]), int(five_landmarks[2, 1])), 3, (0,255,0), -1)\n",
    "            # draw_landmarks(frame_copy, five_landmarks)\n",
    "            for face_obj in faces_list:\n",
    "                gaze_estimator.estimate_gaze(undistorted_frame, face_obj)\n",
    "                pitch_predicted, yaw_predicted = face_obj.normalized_gaze_angles\n",
    "                rot_obj = Rotation.from_euler('xyz', [pitch_predicted, -yaw_predicted, 0], degrees=False)\n",
    "                rot_mat = rot_obj.as_matrix()\n",
    "                end_time = time.time()\n",
    "                estimated_mat = None\n",
    "                raw_rot_eulers.append(rot_obj.as_euler('xyz', degrees=True))\n",
    "                raw_rot_matrices.append(rot_mat)\n",
    "\n",
    "                if prev_rotation==None:\n",
    "                    # angular_velocity = rot_obj.as_euler('yxz')/(deltatime+end_time-start_time)\n",
    "                    kf_obj.statePre = np.array([pitch_predicted, -yaw_predicted, 0, 0], dtype=np.float32)\n",
    "                    kf_obj.processNoiseCov = np.identity(4, dtype=np.float32) * 1e-4\n",
    "                    kf_obj.measurementNoiseCov = np.identity(2, dtype=np.float32) * 1e-1\n",
    "                    kf_obj.errorCovPost = np.identity(4, dtype=np.float32) * 1e-1\n",
    "                    smooth_rot_eulers.append(rot_obj.as_euler('xyz', degrees=True))\n",
    "                    smooth_rot_matrices.append(rot_mat)\n",
    "                else:\n",
    "                    angular_velocity = (rot_obj.as_euler('yxz', \\\n",
    "                                        degrees=False)-prev_rotation.as_euler('yxz', \\\n",
    "                                        degrees=False))/(deltatime+end_time-start_time)\n",
    "                    estimated = kf_obj.correct(np.array([pitch_predicted, -yaw_predicted], dtype=np.float32))\n",
    "                    kf_obj.statePre = np.array([pitch_predicted, -yaw_predicted, angular_velocity[0], angular_velocity[1]], dtype=np.float32)\n",
    "                    predicted = kf_obj.predict()\n",
    "                    estimated_mat = Rotation.from_euler('xyz', np.array([predicted[0], predicted[1], 0.]), degrees=False)\n",
    "                    estrot_mat = estimated_mat.as_matrix()\n",
    "                    smooth_rot_eulers.append(estimated_mat.as_euler('xyz', degrees=True))\n",
    "                    smooth_rot_matrices.append(estrot_mat)\n",
    "                prev_rotation = rot_obj\n",
    "                deltatime = 0\n",
    "\n",
    "                frame_copy = disp_gaze_orthographic(frame_copy, \n",
    "                                                    five_landmarks[2, 0], \n",
    "                                                    five_landmarks[2, 1], \n",
    "                                                    rot_mat, dist=frame_resized.shape[0]//3, color=(0,0,255))\n",
    "                # frame_copy = disp_gaze_ROIperpective(frame_copy, \n",
    "                #                            five_landmarks[2, 0]-raw_bbox[0], \n",
    "                #                            five_landmarks[2, 1]-raw_bbox[1], \n",
    "                #                            raw_bbox[0], raw_bbox[1], raw_bbox[2], raw_bbox[3], \n",
    "                #                            rot_mat, dist=frame_resized.shape[0]//3, color=(0,0,255))\n",
    "                if estimated_mat is not None:\n",
    "                    frame_copy = disp_gaze_orthographic(frame_copy, \n",
    "                                                        five_landmarks[2, 0], \n",
    "                                                        five_landmarks[2, 1], \n",
    "                                                        estrot_mat, dist=frame_resized.shape[0]//3, color=(0,255,0))\n",
    "                    # frame_copy = disp_gaze_ROIperpective(frame_copy, \n",
    "                    #                                     five_landmarks[2, 0]-raw_bbox[0], \n",
    "                    #                                     five_landmarks[2, 1]-raw_bbox[1], \n",
    "                    #                                     raw_bbox[0], raw_bbox[1], raw_bbox[2], raw_bbox[3], \n",
    "                    #                                     estrot_mat, dist=frame_resized.shape[0]//3, color=(0,255,0))\n",
    "        else:\n",
    "            end_time = time.time()\n",
    "            deltatime += end_time-start_time\n",
    "            raw_faces.append(np.zeros((4), dtype=np.float32))\n",
    "            raw_landmarks.append(np.zeros((468, 2), dtype=np.float32))\n",
    "            raw_rot_eulers.append(np.zeros((3,), dtype=float))\n",
    "            raw_rot_matrices.append(np.identity(3, dtype=float))\n",
    "            smooth_rot_eulers.append(np.zeros((3,), dtype=float))\n",
    "            smooth_rot_matrices.append(np.identity(3, dtype=float))\n",
    "\n",
    "        frame_copy = imutils.resize(frame_copy, height=480)\n",
    "        if pose_writer is None:\n",
    "            pose_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),\n",
    "                                         fps/downsample, (frame_copy.shape[1], frame_copy.shape[0]))\n",
    "        pose_writer.write(frame_copy)\n",
    "        cv2.imshow('Video', frame_copy)\n",
    "        ch = cv2.waitKey(1)\n",
    "        if ch<0:\n",
    "            continue\n",
    "        elif chr(ch)=='x':\n",
    "            exit_flag = True\n",
    "            break\n",
    "        elif chr(ch)=='n':\n",
    "            break\n",
    "    \n",
    "    raw_faces = np.stack(raw_faces, axis=0)\n",
    "    raw_landmarks = np.stack(raw_landmarks, axis=0)\n",
    "    raw_rot_eulers = np.stack(raw_rot_eulers, axis=0)\n",
    "    raw_rot_matrices = np.stack(raw_rot_matrices, axis=0)\n",
    "    smooth_rot_eulers = np.stack(smooth_rot_eulers, axis=0)\n",
    "    smooth_rot_matrices = np.stack(smooth_rot_matrices, axis=0)\n",
    "    with open(join(save_dir, vid_prefix+'.pkl'), 'wb') as fp:\n",
    "        pkl.dump({'RAW_GAZE':{'EULER':raw_rot_eulers, 'ROTMAT':raw_rot_matrices},\n",
    "                  'SMOOTH_GAZE':{'EULER':smooth_rot_eulers, 'ROTMAT':smooth_rot_matrices},\n",
    "                  'FACES':raw_faces, 'LANDMARKS':raw_landmarks}, fp)\n",
    "\n",
    "    if pose_writer is not None:\n",
    "        pose_writer.release()\n",
    "        pose_writer = None\n",
    "        ffmpeg_cmd = 'ffmpeg {} -i \"{}\" -i \"{}\" -map 0:v -map 1:a -c:v libx264 -c:a copy -strict -2 \"{}\"'.format(\n",
    "                                                                                '-hide_banner -loglevel error',\n",
    "                                                                                save_path,\n",
    "                                                                                vid_path,\n",
    "                                                                                join(save_dir, 'vid.mp4'))\n",
    "        os.system(ffmpeg_cmd)\n",
    "        os.remove(save_path)\n",
    "        os.rename(join(save_dir, 'vid.mp4'), save_path)\n",
    "    if exit_flag:\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
